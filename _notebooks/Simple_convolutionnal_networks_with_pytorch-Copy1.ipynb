{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# On importe les pacakges necessaire\n",
    "\n",
    "from torchvision import datasets, models, transforms\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "from PIL import Image\n",
    "import torch\n",
    "import time\n",
    "import cv2\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "# telechargement des données\n",
    "# Où les données seront stockées\n",
    "\n",
    "data_path = '/home/cross/Fab2Alex.github.io/Simple_CNN_en_pytorch/'\n",
    "\n",
    "# Training set\n",
    "cifar10 = datasets.CIFAR10(data_path, train=True, download=True)\n",
    "# Val set\n",
    "cifar10_val = datasets.CIFAR10(data_path, train=False, download=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# chargement les données\n",
    "# et transformations des données (en tenseur et normalisées)\n",
    "cifar10 = datasets.CIFAR10(data_path, train=True, download=False,\n",
    "                          transform=transforms.Compose([\n",
    "                              transforms.ToTensor(),\n",
    "                              transforms.Normalize((0.4915, 0.4823, 0.4468),\n",
    "                                                   (0.2470, 0.2435, 0.2616))\n",
    "                          ]))\n",
    "\n",
    "cifar10_val = datasets.CIFAR10(data_path, train=False, download=False,\n",
    "                          transform=transforms.Compose([\n",
    "                              transforms.ToTensor(),\n",
    "                              transforms.Normalize((0.4915, 0.4823, 0.4468),\n",
    "                                                   (0.2470, 0.2435, 0.2616))\n",
    "                          ]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# on prends les données et on les stockent ds une variable en precisant le Batch size\n",
    "train_loader = torch.utils.data.DataLoader(cifar10, batch_size=64, shuffle = False)\n",
    "val_loader = torch.utils.data.DataLoader(cifar10_val, batch_size=64, shuffle = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_names = ['airplane','automobile','bird','cat','deer',\n",
    "               'dog','frog','horse','ship','truck']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor([[[[-1.0531e+00, -1.3072e+00, -1.1960e+00,  ...,  5.1866e-01,\n",
      "            4.2339e-01,  3.5989e-01],\n",
      "          [-1.7358e+00, -1.9899e+00, -1.7041e+00,  ..., -3.7033e-02,\n",
      "           -1.0054e-01, -5.2909e-02],\n",
      "          [-1.5930e+00, -1.7358e+00, -1.2119e+00,  ..., -1.1642e-01,\n",
      "           -8.4663e-02, -2.5931e-01],\n",
      "          ...,\n",
      "          [ 1.3125e+00,  1.2014e+00,  1.1537e+00,  ...,  5.5041e-01,\n",
      "           -1.1008e+00, -1.1484e+00],\n",
      "          [ 8.6794e-01,  7.5681e-01,  9.6321e-01,  ...,  9.3145e-01,\n",
      "           -4.4983e-01, -6.7210e-01],\n",
      "          [ 8.2031e-01,  6.7742e-01,  8.5207e-01,  ...,  1.4395e+00,\n",
      "            4.0752e-01, -3.7033e-02]],\n",
      "\n",
      "         [[-9.8219e-01, -1.2399e+00, -1.2077e+00,  ...,  1.4516e-01,\n",
      "            3.2427e-02,  1.6322e-02],\n",
      "          [-1.6586e+00, -1.9807e+00, -1.8519e+00,  ..., -5.6346e-01,\n",
      "           -6.4398e-01, -5.7956e-01],\n",
      "          [-1.5942e+00, -1.8680e+00, -1.5459e+00,  ..., -6.2788e-01,\n",
      "           -6.2788e-01, -8.0503e-01],\n",
      "          ...,\n",
      "          [ 7.5715e-01,  4.8337e-01,  6.1221e-01,  ...,  1.6127e-01,\n",
      "           -1.4814e+00, -1.4331e+00],\n",
      "          [ 2.5790e-01,  2.1737e-04,  3.3842e-01,  ...,  4.0284e-01,\n",
      "           -9.8219e-01, -1.1271e+00],\n",
      "          [ 3.3842e-01,  9.6848e-02,  3.0621e-01,  ...,  9.8262e-01,\n",
      "           -8.0308e-02, -4.9904e-01]],\n",
      "\n",
      "         [[-7.6354e-01, -1.0334e+00, -1.0634e+00,  ..., -8.8955e-02,\n",
      "           -1.7890e-01, -1.6391e-01],\n",
      "          [-1.4081e+00, -1.7080e+00, -1.7080e+00,  ..., -8.8346e-01,\n",
      "           -9.5842e-01, -8.5348e-01],\n",
      "          [-1.3931e+00, -1.7080e+00, -1.5880e+00,  ..., -9.5842e-01,\n",
      "           -9.5842e-01, -1.0783e+00],\n",
      "          ...,\n",
      "          [-2.6884e-01, -1.1983e+00, -1.3182e+00,  ..., -6.5860e-01,\n",
      "           -1.6030e+00, -1.4081e+00],\n",
      "          [-2.6884e-01, -1.0783e+00, -1.2582e+00,  ..., -2.9882e-01,\n",
      "           -1.1983e+00, -1.1983e+00],\n",
      "          [ 3.0971e-02, -2.9882e-01, -4.0376e-01,  ...,  3.9075e-01,\n",
      "           -4.4873e-01, -6.2862e-01]]],\n",
      "\n",
      "\n",
      "        [[[ 4.5515e-01,  1.0598e-02, -3.2281e-01,  ..., -5.4509e-01,\n",
      "           -6.0860e-01, -7.3561e-01],\n",
      "          [ 2.3287e-01,  3.1226e-01, -5.2790e-03,  ..., -4.6571e-01,\n",
      "           -7.6737e-01, -8.6263e-01],\n",
      "          [ 2.3287e-01,  2.1700e-01, -1.6405e-01,  ..., -7.3561e-01,\n",
      "           -9.1026e-01, -9.2613e-01],\n",
      "          ...,\n",
      "          [ 7.8856e-01,  4.8690e-01,  4.5515e-01,  ..., -1.3231e+00,\n",
      "           -1.0214e+00, -5.1334e-01],\n",
      "          [ 6.2979e-01,  4.8690e-01,  5.3453e-01,  ..., -3.5457e-01,\n",
      "           -3.7033e-02,  8.9982e-02],\n",
      "          [ 5.9804e-01,  5.1866e-01,  5.9804e-01,  ...,  2.8050e-01,\n",
      "            2.8050e-01,  2.8050e-01]],\n",
      "\n",
      "         [[ 8.6989e-01,  2.2569e-01, -3.0578e-01,  ..., -4.5072e-01,\n",
      "           -5.3125e-01, -6.7619e-01],\n",
      "          [ 5.9610e-01,  4.8337e-01,  3.2427e-02,  ..., -3.8630e-01,\n",
      "           -6.9230e-01, -8.0503e-01],\n",
      "          [ 5.1558e-01,  3.7063e-01, -1.2862e-01,  ..., -6.6009e-01,\n",
      "           -8.5335e-01, -8.6945e-01],\n",
      "          ...,\n",
      "          [ 7.0884e-01,  4.9947e-01,  5.9610e-01,  ..., -1.4331e+00,\n",
      "           -1.1271e+00, -6.4398e-01],\n",
      "          [ 4.9947e-01,  4.6726e-01,  6.1221e-01,  ..., -4.8293e-01,\n",
      "           -1.4473e-01, -3.1993e-02],\n",
      "          [ 4.0284e-01,  4.0284e-01,  5.3168e-01,  ...,  1.6127e-01,\n",
      "            1.7737e-01,  1.6127e-01]],\n",
      "\n",
      "         [[ 1.0953e+00,  3.3079e-01, -2.8383e-01,  ..., -6.4361e-01,\n",
      "           -6.4361e-01, -6.5860e-01],\n",
      "          [ 8.2548e-01,  6.0062e-01,  6.0952e-02,  ..., -5.3868e-01,\n",
      "           -7.7853e-01, -7.9352e-01],\n",
      "          [ 7.5052e-01,  5.2566e-01, -2.8992e-02,  ..., -7.4855e-01,\n",
      "           -8.8346e-01, -8.8346e-01],\n",
      "          ...,\n",
      "          [ 7.8051e-01,  6.9056e-01,  8.4047e-01,  ..., -1.1683e+00,\n",
      "           -8.5348e-01, -3.4380e-01],\n",
      "          [ 2.1086e-01,  2.4084e-01,  4.2073e-01,  ..., -2.6884e-01,\n",
      "            9.0934e-02,  2.5583e-01],\n",
      "          [ 9.0934e-02,  1.2092e-01,  2.8581e-01,  ...,  3.7576e-01,\n",
      "            4.2073e-01,  4.5071e-01]]],\n",
      "\n",
      "\n",
      "        [[[ 2.0587e+00,  2.0270e+00,  2.0270e+00,  ...,  2.0270e+00,\n",
      "            2.0270e+00,  2.0270e+00],\n",
      "          [ 2.0587e+00,  2.0587e+00,  2.0587e+00,  ...,  2.0587e+00,\n",
      "            2.0587e+00,  2.0587e+00],\n",
      "          [ 2.0587e+00,  2.0428e+00,  2.0428e+00,  ...,  2.0428e+00,\n",
      "            2.0428e+00,  2.0428e+00],\n",
      "          ...,\n",
      "          [-1.9580e-01, -2.2755e-01, -3.2281e-01,  ..., -8.4675e-01,\n",
      "           -8.4675e-01, -8.4675e-01],\n",
      "          [-2.2755e-01, -3.3869e-01, -4.1808e-01,  ..., -9.1026e-01,\n",
      "           -8.7850e-01, -7.5149e-01],\n",
      "          [-3.0694e-01, -4.1808e-01, -4.8158e-01,  ..., -7.5149e-01,\n",
      "           -7.3561e-01, -7.1973e-01]],\n",
      "\n",
      "         [[ 2.1261e+00,  2.0939e+00,  2.0939e+00,  ...,  2.0939e+00,\n",
      "            2.0939e+00,  2.0939e+00],\n",
      "          [ 2.1261e+00,  2.1261e+00,  2.1261e+00,  ...,  2.1261e+00,\n",
      "            2.1261e+00,  2.1261e+00],\n",
      "          [ 2.1261e+00,  2.1100e+00,  2.1100e+00,  ...,  2.1100e+00,\n",
      "            2.1100e+00,  2.1100e+00],\n",
      "          ...,\n",
      "          [-4.8098e-02, -8.0308e-02, -1.7694e-01,  ..., -6.7619e-01,\n",
      "           -6.9230e-01, -6.9230e-01],\n",
      "          [-8.0308e-02, -1.9304e-01, -2.7357e-01,  ..., -7.7282e-01,\n",
      "           -7.5672e-01, -6.2788e-01],\n",
      "          [-1.6083e-01, -2.7357e-01, -3.3799e-01,  ..., -6.1177e-01,\n",
      "           -6.1177e-01, -5.9567e-01]],\n",
      "\n",
      "         [[ 2.1147e+00,  2.0847e+00,  2.0847e+00,  ...,  2.0847e+00,\n",
      "            2.0847e+00,  2.0847e+00],\n",
      "          [ 2.1147e+00,  2.1147e+00,  2.1147e+00,  ...,  2.1147e+00,\n",
      "            2.1147e+00,  2.1147e+00],\n",
      "          [ 2.1147e+00,  2.0997e+00,  2.0997e+00,  ...,  2.0997e+00,\n",
      "            2.0997e+00,  2.0997e+00],\n",
      "          ...,\n",
      "          [-2.8992e-02, -4.3983e-02, -1.1894e-01,  ..., -5.0869e-01,\n",
      "           -5.2369e-01, -5.2369e-01],\n",
      "          [-5.8973e-02, -1.4892e-01, -2.3886e-01,  ..., -6.1363e-01,\n",
      "           -5.8365e-01, -4.7871e-01],\n",
      "          [-1.3393e-01, -2.3886e-01, -2.9882e-01,  ..., -4.6372e-01,\n",
      "           -4.6372e-01, -4.4873e-01]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[-1.7992e-01, -1.3229e-01, -8.4663e-02,  ...,  6.2979e-01,\n",
      "           -5.2790e-03, -3.5457e-01],\n",
      "          [-2.2755e-01, -1.4817e-01, -6.8786e-02,  ...,  5.8216e-01,\n",
      "            2.6475e-02, -3.5457e-01],\n",
      "          [-1.7992e-01, -6.8786e-02,  4.2352e-02,  ...,  6.6155e-01,\n",
      "            1.0586e-01, -3.0694e-01],\n",
      "          ...,\n",
      "          [ 6.2979e-01,  7.0918e-01,  7.8856e-01,  ...,  9.4733e-01,\n",
      "            1.2966e+00,  1.2014e+00],\n",
      "          [ 7.8856e-01,  7.8856e-01,  8.6794e-01,  ...,  9.7908e-01,\n",
      "            1.1855e+00,  1.0743e+00],\n",
      "          [ 7.5681e-01,  7.2505e-01,  8.2031e-01,  ...,  1.2649e+00,\n",
      "            1.3442e+00,  1.2172e+00]],\n",
      "\n",
      "         [[-6.4203e-02, -1.5888e-02,  4.8532e-02,  ...,  6.9273e-01,\n",
      "            4.8532e-02, -3.2188e-01],\n",
      "          [-1.1252e-01, -3.1993e-02,  4.8532e-02,  ...,  6.4442e-01,\n",
      "            8.0743e-02, -3.2188e-01],\n",
      "          [-6.4203e-02,  3.2427e-02,  1.4516e-01,  ...,  7.2494e-01,\n",
      "            1.6127e-01, -2.4136e-01],\n",
      "          ...,\n",
      "          [ 6.9273e-01,  7.7326e-01,  8.5378e-01,  ...,  9.0210e-01,\n",
      "            1.2242e+00,  1.1276e+00],\n",
      "          [ 8.5378e-01,  8.5378e-01,  9.3431e-01,  ...,  9.5041e-01,\n",
      "            1.1437e+00,  1.0148e+00],\n",
      "          [ 8.2157e-01,  7.8936e-01,  8.8599e-01,  ...,  1.2403e+00,\n",
      "            1.3047e+00,  1.1598e+00]],\n",
      "\n",
      "         [[ 1.6589e-01,  1.8088e-01,  2.2585e-01,  ...,  6.9056e-01,\n",
      "            1.0592e-01, -5.8973e-02],\n",
      "          [ 1.2092e-01,  1.9587e-01,  2.8581e-01,  ...,  6.4559e-01,\n",
      "            1.3591e-01, -7.3964e-02],\n",
      "          [ 1.6589e-01,  3.3079e-01,  4.2073e-01,  ...,  7.2054e-01,\n",
      "            1.8088e-01, -1.7890e-01],\n",
      "          ...,\n",
      "          [ 7.0555e-01,  7.9550e-01,  8.5546e-01,  ...,  8.1049e-01,\n",
      "            1.2002e+00,  1.0653e+00],\n",
      "          [ 8.5546e-01,  8.5546e-01,  9.3041e-01,  ...,  7.2054e-01,\n",
      "            1.0054e+00,  9.6039e-01],\n",
      "          [ 8.2548e-01,  7.9550e-01,  8.8544e-01,  ...,  1.0204e+00,\n",
      "            1.1703e+00,  1.0953e+00]]],\n",
      "\n",
      "\n",
      "        [[[-7.8324e-01, -8.3087e-01, -8.9438e-01,  ..., -7.9912e-01,\n",
      "           -8.4675e-01, -8.7850e-01],\n",
      "          [-7.8324e-01, -8.6263e-01, -9.4201e-01,  ..., -7.3561e-01,\n",
      "           -7.8324e-01, -7.9912e-01],\n",
      "          [-8.6263e-01, -9.5789e-01, -1.0055e+00,  ..., -7.1973e-01,\n",
      "           -7.6737e-01, -7.8324e-01],\n",
      "          ...,\n",
      "          [-1.8152e+00, -1.8470e+00, -1.8946e+00,  ..., -1.4977e+00,\n",
      "           -1.4818e+00, -1.5295e+00],\n",
      "          [-1.9899e+00, -1.9899e+00, -1.9899e+00,  ..., -1.7994e+00,\n",
      "           -1.7994e+00, -1.7835e+00],\n",
      "          [-6.0860e-01, -6.7210e-01, -7.0386e-01,  ..., -4.1808e-01,\n",
      "           -4.1808e-01, -3.7045e-01]],\n",
      "\n",
      "         [[-8.0308e-02, -8.0308e-02, -1.1252e-01,  ...,  1.9348e-01,\n",
      "            1.7737e-01,  1.9348e-01],\n",
      "          [-1.5888e-02, -6.4203e-02, -9.6413e-02,  ...,  2.0958e-01,\n",
      "            1.9348e-01,  2.2569e-01],\n",
      "          [-4.8098e-02, -9.6413e-02, -1.1252e-01,  ...,  1.6127e-01,\n",
      "            1.4516e-01,  1.9348e-01],\n",
      "          ...,\n",
      "          [-1.4009e+00, -1.4653e+00, -1.4653e+00,  ..., -9.9829e-01,\n",
      "           -9.8219e-01, -1.0305e+00],\n",
      "          [-1.6747e+00, -1.7713e+00, -1.8680e+00,  ..., -1.3687e+00,\n",
      "           -1.3848e+00, -1.3687e+00],\n",
      "          [-3.7020e-01, -4.9904e-01, -5.9567e-01,  ..., -1.1252e-01,\n",
      "           -1.2862e-01, -6.4203e-02]],\n",
      "\n",
      "         [[ 7.9550e-01,  7.5052e-01,  7.2054e-01,  ...,  9.9038e-01,\n",
      "            9.6039e-01,  9.7539e-01],\n",
      "          [ 8.4047e-01,  7.8051e-01,  7.3553e-01,  ...,  9.6039e-01,\n",
      "            9.4540e-01,  9.7539e-01],\n",
      "          [ 8.4047e-01,  7.6552e-01,  7.5052e-01,  ...,  8.8544e-01,\n",
      "            8.7045e-01,  9.0043e-01],\n",
      "          ...,\n",
      "          [-7.1856e-01, -7.9352e-01, -7.9352e-01,  ..., -5.2369e-01,\n",
      "           -5.0869e-01, -5.5367e-01],\n",
      "          [-1.0184e+00, -1.1083e+00, -1.1683e+00,  ..., -8.8346e-01,\n",
      "           -8.8346e-01, -8.6847e-01],\n",
      "          [ 1.5980e-02, -8.8955e-02, -1.6391e-01,  ...,  1.9587e-01,\n",
      "            1.9587e-01,  2.4084e-01]]],\n",
      "\n",
      "\n",
      "        [[[-9.5789e-01, -8.7850e-01, -7.8324e-01,  ..., -1.4659e+00,\n",
      "           -1.0531e+00, -1.0055e+00],\n",
      "          [-8.6263e-01, -7.5149e-01, -6.0860e-01,  ..., -1.6247e+00,\n",
      "           -9.5789e-01, -8.1500e-01],\n",
      "          [-8.9438e-01, -7.6737e-01, -6.8798e-01,  ..., -1.6882e+00,\n",
      "           -9.2613e-01, -6.7210e-01],\n",
      "          ...,\n",
      "          [-1.7676e+00, -1.7676e+00, -1.7676e+00,  ..., -1.1325e+00,\n",
      "           -1.1325e+00, -1.1643e+00],\n",
      "          [-1.7676e+00, -1.7676e+00, -1.7676e+00,  ..., -1.1484e+00,\n",
      "           -1.2754e+00, -1.3866e+00],\n",
      "          [-1.7676e+00, -1.7676e+00, -1.7676e+00,  ..., -1.2119e+00,\n",
      "           -1.4977e+00, -1.6565e+00]],\n",
      "\n",
      "         [[-1.4473e-01, -3.1993e-02,  9.6848e-02,  ..., -1.2238e+00,\n",
      "           -9.8219e-01, -9.1777e-01],\n",
      "          [-4.8098e-02,  9.6848e-02,  2.7400e-01,  ..., -1.4975e+00,\n",
      "           -9.8219e-01, -8.2114e-01],\n",
      "          [-8.0308e-02,  9.6848e-02,  2.2569e-01,  ..., -1.6586e+00,\n",
      "           -1.0627e+00, -8.0503e-01],\n",
      "          ...,\n",
      "          [-1.7552e+00, -1.7552e+00, -1.7552e+00,  ..., -7.5672e-01,\n",
      "           -6.9230e-01, -6.7619e-01],\n",
      "          [-1.7552e+00, -1.7552e+00, -1.7552e+00,  ..., -7.5672e-01,\n",
      "           -8.6945e-01, -9.6608e-01],\n",
      "          [-1.7552e+00, -1.7552e+00, -1.7552e+00,  ..., -8.2114e-01,\n",
      "           -1.1593e+00, -1.3526e+00]],\n",
      "\n",
      "         [[-8.9845e-01, -7.7853e-01, -6.4361e-01,  ..., -1.0783e+00,\n",
      "           -9.1344e-01, -8.6847e-01],\n",
      "          [-8.5348e-01, -7.3356e-01, -5.8365e-01,  ..., -1.3482e+00,\n",
      "           -9.4343e-01, -8.5348e-01],\n",
      "          [-9.1344e-01, -8.0851e-01, -7.6354e-01,  ..., -1.5131e+00,\n",
      "           -9.7341e-01, -7.1856e-01],\n",
      "          ...,\n",
      "          [-1.4981e+00, -1.4981e+00, -1.4981e+00,  ..., -1.0334e+00,\n",
      "           -1.0783e+00, -1.0184e+00],\n",
      "          [-1.4981e+00, -1.4981e+00, -1.4981e+00,  ..., -1.0184e+00,\n",
      "           -1.1533e+00, -1.1983e+00],\n",
      "          [-1.4981e+00, -1.4981e+00, -1.4981e+00,  ..., -1.0334e+00,\n",
      "           -1.2582e+00, -1.3931e+00]]]]), tensor([6, 9, 9, 4, 1, 1, 2, 7, 8, 3, 4, 7, 7, 2, 9, 9, 9, 3, 2, 6, 4, 3, 6, 6,\n",
      "        2, 6, 3, 5, 4, 0, 0, 9, 1, 3, 4, 0, 3, 7, 3, 3, 5, 2, 2, 7, 1, 1, 1, 2,\n",
      "        2, 0, 9, 5, 7, 9, 2, 2, 5, 2, 4, 3, 1, 1, 8, 2])]\n"
     ]
    }
   ],
   "source": [
    "# À quoi ressemblent nos données?\n",
    "for i,data in enumerate(train_loader):\n",
    "    print(data)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.8352, 0.4473, 0.0355, 0.9228, 0.3016],\n",
      "        [0.5993, 0.8486, 0.0842, 0.9758, 0.8946]])\n"
     ]
    }
   ],
   "source": [
    "# petit rappel de resizing avec pytorch.\n",
    "y = torch.rand([2,5])\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.8352, 0.4473, 0.0355, 0.9228, 0.3016, 0.5993, 0.8486, 0.0842, 0.9758,\n",
       "         0.8946]])"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# le 1 veut dire je veux que tu transforme mon tenseur en tenseur de dimension 1\n",
    "# le -1 signifie, quelques soit le nombre de colonne et de raws, fait en sorte que ca fonctionne\n",
    "y.view(1, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.8589, 0.9774, 0.1267, 0.4872, 0.6009],\n",
      "        [0.9181, 0.4921, 0.8708, 0.4466, 0.8675],\n",
      "        [0.9320, 0.3127, 0.9883, 0.2780, 0.3676],\n",
      "        [0.2537, 0.2133, 0.6165, 0.5572, 0.9587],\n",
      "        [0.4082, 0.3598, 0.3268, 0.8912, 0.4331],\n",
      "        [0.4744, 0.4663, 0.8735, 0.6122, 0.0397],\n",
      "        [0.6109, 0.1888, 0.2805, 0.3954, 0.2455],\n",
      "        [0.0237, 0.3705, 0.7930, 0.1520, 0.1921],\n",
      "        [0.6145, 0.2284, 0.1437, 0.8138, 0.2041],\n",
      "        [0.9346, 0.6814, 0.1385, 0.4030, 0.6243],\n",
      "        [0.2064, 0.9423, 0.2886, 0.8816, 0.9937],\n",
      "        [0.7003, 0.7847, 0.5340, 0.3842, 0.4546],\n",
      "        [0.0722, 0.7205, 0.4217, 0.0499, 0.3765],\n",
      "        [0.0804, 0.4982, 0.7653, 0.7028, 0.7902],\n",
      "        [0.6298, 0.2399, 0.3113, 0.6215, 0.9448],\n",
      "        [0.5032, 0.9564, 0.8843, 0.1366, 0.3695],\n",
      "        [0.3311, 0.1262, 0.4707, 0.9563, 0.6541]])\n"
     ]
    }
   ],
   "source": [
    "#autre exemple\n",
    "x = torch.rand([17,5])\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.8589, 0.9774, 0.1267, 0.4872, 0.6009, 0.9181, 0.4921, 0.8708, 0.4466,\n",
       "         0.8675, 0.9320, 0.3127, 0.9883, 0.2780, 0.3676, 0.2537, 0.2133, 0.6165,\n",
       "         0.5572, 0.9587, 0.4082, 0.3598, 0.3268, 0.8912, 0.4331, 0.4744, 0.4663,\n",
       "         0.8735, 0.6122, 0.0397, 0.6109, 0.1888, 0.2805, 0.3954, 0.2455, 0.0237,\n",
       "         0.3705, 0.7930, 0.1520, 0.1921, 0.6145, 0.2284, 0.1437, 0.8138, 0.2041,\n",
       "         0.9346, 0.6814, 0.1385, 0.4030, 0.6243, 0.2064, 0.9423, 0.2886, 0.8816,\n",
       "         0.9937, 0.7003, 0.7847, 0.5340, 0.3842, 0.4546, 0.0722, 0.7205, 0.4217,\n",
       "         0.0499, 0.3765, 0.0804, 0.4982, 0.7653, 0.7028, 0.7902, 0.6298, 0.2399,\n",
       "         0.3113, 0.6215, 0.9448, 0.5032, 0.9564, 0.8843, 0.1366, 0.3695, 0.3311,\n",
       "         0.1262, 0.4707, 0.9563, 0.6541]])"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.view(1, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# connaitre la taille des inputs de sortie des maxpool par exemple avant la FC\n",
    "# Contrairement aux couches conv dans pyTorch, où l'on a pas besoin de spécifier la taille d'entrée, \n",
    "# on doit le spécifier pour les couches entièrement connectées (FC layers).\n",
    "# Ainsi, lorsque vous définissez la dimension d'entrée de la première couche linéaire, \n",
    "# vous devez savoir quelle est la taille des images que vous alimentez\n",
    "\n",
    "class Print(nn.Module):\n",
    "    def forward(self, x):\n",
    "        #print((x.size())\n",
    "        return x.view(x.size(0),-1)\n",
    "a = Print()    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Brain(\n",
      "  (conv1): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (act1): Tanh()\n",
      "  (pool1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (conv2): Conv2d(32, 6, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (act2): Tanh()\n",
      "  (pool2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (fc1): Linear(in_features=384, out_features=32, bias=True)\n",
      "  (act3): Tanh()\n",
      "  (fc2): Linear(in_features=32, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# On reprend un reseau de neurones simple mais en y ajoutant des convolutions\n",
    "class Brain(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # ds cet ordre pr la Conv2D -> channel, filtres (nombre d outputs features) \n",
    "        self.conv1 = nn.Conv2d(3, 32, kernel_size=3, padding=1)\n",
    "        self.act1 = nn.Tanh()\n",
    "        self.pool1 = nn.MaxPool2d(2)\n",
    "        # 6 represente le nbre de feature en output\n",
    "        self.conv2 = nn.Conv2d(32, 6, kernel_size=3, padding=1)\n",
    "        self.act2 = nn.Tanh()\n",
    "        self.pool2 = nn.MaxPool2d(2)\n",
    "        # \n",
    "        self.fc1 = nn.Linear(384, 32)\n",
    "        self.act3 = nn.Tanh()\n",
    "        self.fc2 = nn.Linear(32, 10)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "        out = self.pool1(self.act1(self.conv1(x)))\n",
    "        out = self.pool2(self.act2(self.conv2(out)))\n",
    "        \n",
    "        # D'où vient ce 8*8*6\n",
    "        # En keras ou tensorflow il exist la methode flatten\n",
    "        # Mais comment faire en pytorch?\n",
    "        # source: http://cs231n.github.io/convolutional-networks/#pool\n",
    "        # https://pytorch.org/docs/stable/nn.html?highlight=maxpool2#torch.nn.Conv2d\n",
    "        # https://pytorch.org/docs/stable/nn.html?highlight=maxpool2#torch.nn.MaxPool2d\n",
    "        \n",
    "        out = a.forward(out)\n",
    "        print(out.shape)\n",
    "        \n",
    "        #out = out.view(-1, 8*8*6)\n",
    "        #a.forward(out)\n",
    "        out = self.act3(self.fc1(out))\n",
    "        out = self.fc2(out)\n",
    "        return out\n",
    "\n",
    "model = Brain()\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "\n",
    "def training_loop(n_epochs, optimizer, model, loss_fn, train_loader):\n",
    "    for epoch in range(1, n_epochs + 1):\n",
    "        loss_train = 0.0\n",
    "        for imgs, labels in train_loader:\n",
    "            outputs = model(imgs)\n",
    "            loss = loss_fn(outputs, labels)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            loss_train += loss.item()\n",
    "            \n",
    "        \n",
    "        print('{} Epoch {}, Training loss {}'.format(\n",
    "            datetime.datetime.now(), epoch, float(loss_train)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Brain()\n",
    "optimizer = optim.SGD(model.parameters(), lr=1e-2)\n",
    "loss_fn = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 384])\n",
      "torch.Size([64, 384])\n",
      "torch.Size([64, 384])\n",
      "torch.Size([64, 384])\n",
      "torch.Size([64, 384])\n",
      "torch.Size([64, 384])\n",
      "torch.Size([64, 384])\n",
      "torch.Size([64, 384])\n",
      "torch.Size([64, 384])\n",
      "torch.Size([64, 384])\n",
      "torch.Size([64, 384])\n",
      "torch.Size([64, 384])\n",
      "torch.Size([64, 384])\n",
      "torch.Size([64, 384])\n",
      "torch.Size([64, 384])\n",
      "torch.Size([64, 384])\n",
      "torch.Size([64, 384])\n",
      "torch.Size([64, 384])\n",
      "torch.Size([64, 384])\n",
      "torch.Size([64, 384])\n",
      "torch.Size([64, 384])\n",
      "torch.Size([64, 384])\n",
      "torch.Size([64, 384])\n",
      "torch.Size([64, 384])\n",
      "torch.Size([64, 384])\n",
      "torch.Size([64, 384])\n",
      "torch.Size([64, 384])\n",
      "torch.Size([64, 384])\n",
      "torch.Size([64, 384])\n",
      "torch.Size([64, 384])\n",
      "torch.Size([64, 384])\n",
      "torch.Size([64, 384])\n",
      "torch.Size([64, 384])\n",
      "torch.Size([64, 384])\n",
      "torch.Size([64, 384])\n",
      "torch.Size([64, 384])\n",
      "torch.Size([64, 384])\n",
      "torch.Size([64, 384])\n",
      "torch.Size([64, 384])\n",
      "torch.Size([64, 384])\n",
      "torch.Size([64, 384])\n",
      "torch.Size([64, 384])\n",
      "torch.Size([64, 384])\n",
      "torch.Size([64, 384])\n",
      "torch.Size([64, 384])\n",
      "torch.Size([64, 384])\n",
      "torch.Size([64, 384])\n",
      "torch.Size([64, 384])\n",
      "torch.Size([64, 384])\n",
      "torch.Size([64, 384])\n",
      "torch.Size([64, 384])\n",
      "torch.Size([64, 384])\n",
      "torch.Size([64, 384])\n",
      "torch.Size([64, 384])\n",
      "torch.Size([64, 384])\n",
      "torch.Size([64, 384])\n",
      "torch.Size([64, 384])\n",
      "torch.Size([64, 384])\n",
      "torch.Size([64, 384])\n",
      "torch.Size([64, 384])\n",
      "torch.Size([64, 384])\n",
      "torch.Size([64, 384])\n",
      "torch.Size([64, 384])\n",
      "torch.Size([64, 384])\n",
      "torch.Size([64, 384])\n",
      "torch.Size([64, 384])\n",
      "torch.Size([64, 384])\n",
      "torch.Size([64, 384])\n",
      "torch.Size([64, 384])\n",
      "torch.Size([64, 384])\n",
      "torch.Size([64, 384])\n",
      "torch.Size([64, 384])\n",
      "torch.Size([64, 384])\n",
      "torch.Size([64, 384])\n",
      "torch.Size([64, 384])\n",
      "torch.Size([64, 384])\n",
      "torch.Size([64, 384])\n",
      "torch.Size([64, 384])\n",
      "torch.Size([64, 384])\n",
      "torch.Size([64, 384])\n",
      "torch.Size([64, 384])\n",
      "torch.Size([64, 384])\n",
      "torch.Size([64, 384])\n",
      "torch.Size([64, 384])\n",
      "torch.Size([64, 384])\n",
      "torch.Size([64, 384])\n",
      "torch.Size([64, 384])\n",
      "torch.Size([64, 384])\n",
      "torch.Size([64, 384])\n",
      "torch.Size([64, 384])\n",
      "torch.Size([64, 384])\n",
      "torch.Size([64, 384])\n",
      "torch.Size([64, 384])\n",
      "torch.Size([64, 384])\n",
      "torch.Size([64, 384])\n",
      "torch.Size([64, 384])\n",
      "torch.Size([64, 384])\n",
      "torch.Size([64, 384])\n",
      "torch.Size([64, 384])\n",
      "torch.Size([64, 384])\n",
      "torch.Size([64, 384])\n",
      "torch.Size([64, 384])\n",
      "torch.Size([64, 384])\n",
      "torch.Size([64, 384])\n",
      "torch.Size([64, 384])\n",
      "torch.Size([64, 384])\n",
      "torch.Size([64, 384])\n",
      "torch.Size([64, 384])\n",
      "torch.Size([64, 384])\n",
      "torch.Size([64, 384])\n",
      "torch.Size([64, 384])\n",
      "torch.Size([64, 384])\n",
      "torch.Size([64, 384])\n",
      "torch.Size([64, 384])\n",
      "torch.Size([64, 384])\n",
      "torch.Size([64, 384])\n",
      "torch.Size([64, 384])\n",
      "torch.Size([64, 384])\n",
      "torch.Size([64, 384])\n",
      "torch.Size([64, 384])\n",
      "torch.Size([64, 384])\n",
      "torch.Size([64, 384])\n",
      "torch.Size([64, 384])\n",
      "torch.Size([64, 384])\n",
      "torch.Size([64, 384])\n",
      "torch.Size([64, 384])\n",
      "torch.Size([64, 384])\n",
      "torch.Size([64, 384])\n",
      "torch.Size([64, 384])\n",
      "torch.Size([64, 384])\n",
      "torch.Size([64, 384])\n",
      "torch.Size([64, 384])\n",
      "torch.Size([64, 384])\n",
      "torch.Size([64, 384])\n",
      "torch.Size([64, 384])\n",
      "torch.Size([64, 384])\n",
      "torch.Size([64, 384])\n",
      "torch.Size([64, 384])\n",
      "torch.Size([64, 384])\n",
      "torch.Size([64, 384])\n",
      "torch.Size([64, 384])\n",
      "torch.Size([64, 384])\n",
      "torch.Size([64, 384])\n",
      "torch.Size([64, 384])\n",
      "torch.Size([64, 384])\n",
      "torch.Size([64, 384])\n",
      "torch.Size([64, 384])\n",
      "torch.Size([64, 384])\n",
      "torch.Size([64, 384])\n",
      "torch.Size([64, 384])\n",
      "torch.Size([64, 384])\n",
      "torch.Size([64, 384])\n",
      "torch.Size([64, 384])\n",
      "torch.Size([64, 384])\n",
      "torch.Size([64, 384])\n",
      "torch.Size([64, 384])\n",
      "torch.Size([64, 384])\n",
      "torch.Size([64, 384])\n",
      "torch.Size([64, 384])\n",
      "torch.Size([64, 384])\n",
      "torch.Size([64, 384])\n",
      "torch.Size([64, 384])\n",
      "torch.Size([64, 384])\n",
      "torch.Size([64, 384])\n",
      "torch.Size([64, 384])\n",
      "torch.Size([64, 384])\n",
      "torch.Size([64, 384])\n",
      "torch.Size([64, 384])\n",
      "torch.Size([64, 384])\n",
      "torch.Size([64, 384])\n",
      "torch.Size([64, 384])\n",
      "torch.Size([64, 384])\n",
      "torch.Size([64, 384])\n",
      "torch.Size([64, 384])\n",
      "torch.Size([64, 384])\n",
      "torch.Size([64, 384])\n",
      "torch.Size([64, 384])\n",
      "torch.Size([64, 384])\n",
      "torch.Size([64, 384])\n",
      "torch.Size([64, 384])\n",
      "torch.Size([64, 384])\n",
      "torch.Size([64, 384])\n",
      "torch.Size([64, 384])\n",
      "torch.Size([64, 384])\n",
      "torch.Size([64, 384])\n",
      "torch.Size([64, 384])\n",
      "torch.Size([64, 384])\n",
      "torch.Size([64, 384])\n",
      "torch.Size([64, 384])\n",
      "torch.Size([64, 384])\n",
      "torch.Size([64, 384])\n",
      "torch.Size([64, 384])\n",
      "torch.Size([64, 384])\n",
      "torch.Size([64, 384])\n",
      "torch.Size([64, 384])\n",
      "torch.Size([64, 384])\n",
      "torch.Size([64, 384])\n",
      "torch.Size([64, 384])\n",
      "torch.Size([64, 384])\n",
      "torch.Size([64, 384])\n",
      "torch.Size([64, 384])\n",
      "torch.Size([64, 384])\n",
      "torch.Size([64, 384])\n",
      "torch.Size([64, 384])\n",
      "torch.Size([64, 384])\n",
      "torch.Size([64, 384])\n",
      "torch.Size([64, 384])\n",
      "torch.Size([64, 384])\n",
      "torch.Size([64, 384])\n",
      "torch.Size([64, 384])\n",
      "torch.Size([64, 384])\n",
      "torch.Size([64, 384])\n",
      "torch.Size([64, 384])\n",
      "torch.Size([64, 384])\n",
      "torch.Size([64, 384])\n",
      "torch.Size([64, 384])\n",
      "torch.Size([64, 384])\n",
      "torch.Size([64, 384])\n",
      "torch.Size([64, 384])\n",
      "torch.Size([64, 384])\n",
      "torch.Size([64, 384])\n",
      "torch.Size([64, 384])\n",
      "torch.Size([64, 384])\n",
      "torch.Size([64, 384])\n",
      "torch.Size([64, 384])\n",
      "torch.Size([64, 384])\n",
      "torch.Size([64, 384])\n",
      "torch.Size([64, 384])\n",
      "torch.Size([64, 384])\n",
      "torch.Size([64, 384])\n",
      "torch.Size([64, 384])\n",
      "torch.Size([64, 384])\n",
      "torch.Size([64, 384])\n",
      "torch.Size([64, 384])\n",
      "torch.Size([64, 384])\n",
      "torch.Size([64, 384])\n",
      "torch.Size([64, 384])\n",
      "torch.Size([64, 384])\n",
      "torch.Size([64, 384])\n",
      "torch.Size([64, 384])\n",
      "torch.Size([64, 384])\n",
      "torch.Size([64, 384])\n",
      "torch.Size([64, 384])\n",
      "torch.Size([64, 384])\n",
      "torch.Size([64, 384])\n",
      "torch.Size([64, 384])\n",
      "torch.Size([64, 384])\n",
      "torch.Size([64, 384])\n",
      "torch.Size([64, 384])\n",
      "torch.Size([64, 384])\n",
      "torch.Size([64, 384])\n",
      "torch.Size([64, 384])\n",
      "torch.Size([64, 384])\n",
      "torch.Size([64, 384])\n",
      "torch.Size([64, 384])\n",
      "torch.Size([64, 384])\n",
      "torch.Size([64, 384])\n",
      "torch.Size([64, 384])\n",
      "torch.Size([64, 384])\n",
      "torch.Size([64, 384])\n",
      "torch.Size([64, 384])\n",
      "torch.Size([64, 384])\n",
      "torch.Size([64, 384])\n",
      "torch.Size([64, 384])\n",
      "torch.Size([64, 384])\n",
      "torch.Size([64, 384])\n",
      "torch.Size([64, 384])\n",
      "torch.Size([64, 384])\n",
      "torch.Size([64, 384])\n",
      "torch.Size([64, 384])\n",
      "torch.Size([64, 384])\n",
      "torch.Size([64, 384])\n",
      "torch.Size([64, 384])\n",
      "torch.Size([64, 384])\n",
      "torch.Size([64, 384])\n",
      "torch.Size([64, 384])\n",
      "torch.Size([64, 384])\n",
      "torch.Size([64, 384])\n",
      "torch.Size([64, 384])\n",
      "torch.Size([64, 384])\n",
      "torch.Size([64, 384])\n",
      "torch.Size([64, 384])\n",
      "torch.Size([64, 384])\n",
      "torch.Size([64, 384])\n",
      "torch.Size([64, 384])\n",
      "torch.Size([64, 384])\n",
      "torch.Size([64, 384])\n",
      "torch.Size([64, 384])\n",
      "torch.Size([64, 384])\n",
      "torch.Size([64, 384])\n",
      "torch.Size([64, 384])\n",
      "torch.Size([64, 384])\n",
      "torch.Size([64, 384])\n",
      "torch.Size([64, 384])\n",
      "torch.Size([64, 384])\n",
      "torch.Size([64, 384])\n",
      "torch.Size([64, 384])\n",
      "torch.Size([64, 384])\n",
      "torch.Size([64, 384])\n",
      "torch.Size([64, 384])\n",
      "torch.Size([64, 384])\n",
      "torch.Size([64, 384])\n",
      "torch.Size([64, 384])\n",
      "torch.Size([64, 384])\n",
      "torch.Size([64, 384])\n",
      "torch.Size([64, 384])\n",
      "torch.Size([64, 384])\n",
      "torch.Size([64, 384])\n",
      "torch.Size([64, 384])\n",
      "torch.Size([64, 384])\n",
      "torch.Size([64, 384])\n",
      "torch.Size([64, 384])\n",
      "torch.Size([64, 384])\n",
      "torch.Size([64, 384])\n",
      "torch.Size([64, 384])\n",
      "torch.Size([64, 384])\n",
      "torch.Size([64, 384])\n",
      "torch.Size([64, 384])\n",
      "torch.Size([64, 384])\n",
      "torch.Size([64, 384])\n",
      "torch.Size([64, 384])\n",
      "torch.Size([64, 384])\n",
      "torch.Size([64, 384])\n",
      "torch.Size([64, 384])\n",
      "torch.Size([64, 384])\n",
      "torch.Size([64, 384])\n",
      "torch.Size([64, 384])\n",
      "torch.Size([64, 384])\n",
      "torch.Size([64, 384])\n",
      "torch.Size([64, 384])\n",
      "torch.Size([64, 384])\n",
      "torch.Size([64, 384])\n",
      "torch.Size([64, 384])\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-73-42b9ca1cf45d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtraining_loop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_epochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0moptimizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mloss_fn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtrain_loader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-71-ffff1992e29a>\u001b[0m in \u001b[0;36mtraining_loop\u001b[0;34m(n_epochs, optimizer, model, loss_fn, train_loader)\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_epochs\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0mloss_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mimgs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimgs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/fastai/lib/python3.6/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    344\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__next__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    345\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 346\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    347\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    348\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/fastai/lib/python3.6/site-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/fastai/lib/python3.6/site-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/fastai/lib/python3.6/site-packages/torchvision/datasets/cifar.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m    123\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 125\u001b[0;31m             \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    126\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtarget_transform\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/fastai/lib/python3.6/site-packages/torchvision/transforms/transforms.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, img)\u001b[0m\n\u001b[1;32m     68\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransforms\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m             \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/fastai/lib/python3.6/site-packages/torchvision/transforms/transforms.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, tensor)\u001b[0m\n\u001b[1;32m    173\u001b[0m             \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mNormalized\u001b[0m \u001b[0mTensor\u001b[0m \u001b[0mimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    174\u001b[0m         \"\"\"\n\u001b[0;32m--> 175\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnormalize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minplace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    176\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    177\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__repr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/fastai/lib/python3.6/site-packages/torchvision/transforms/functional.py\u001b[0m in \u001b[0;36mnormalize\u001b[0;34m(tensor, mean, std, inplace)\u001b[0m\n\u001b[1;32m    215\u001b[0m     \u001b[0mmean\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    216\u001b[0m     \u001b[0mstd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 217\u001b[0;31m     \u001b[0mtensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msub_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdiv_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstd\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    218\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mtensor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    219\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "training_loop(n_epochs = 100,optimizer = optimizer,model = model,loss_fn = loss_fn,train_loader = train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for loader in [train_loader, val_loader]:\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for imgs, labels in loader:\n",
    "            outputs = model(imgs)\n",
    "            _, predicted = torch.max(outputs, dim=1)\n",
    "            total += labels.shape[0]\n",
    "            correct += int((predicted == labels).sum())\n",
    "    \n",
    "    print(\"Accuracy: %f\" % (correct / total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), data_path + 'CIFAR_10.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# faire une prediction sur une image non vue et printer cette prediction\n",
    "model_entraine = Brain()\n",
    "model_entraine.load_state_dict(torch.load(data_path + 'CIFAR_10.pt'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Le même modèle mais avec un resnet block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# definition dun block Resnet\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# On reprend un reseau de neurones simple mais en y ajoutant des convolutions\n",
    "class BrainALaResnet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # ds cet ordre pr la Conv2D -> channel, filtres (nombre d outputs features) \n",
    "        self.conv1 = nn.Conv2d(3, 32, kernel_size=3, padding=1)\n",
    "        self.act1 = nn.Tanh()\n",
    "        self.pool1 = nn.MaxPool2d(2)\n",
    "        self.conv2 = nn.Conv2d(32, 8, kernel_size=3, padding=1)\n",
    "        self.act2 = nn.Tanh()\n",
    "        self.pool2 = nn.MaxPool2d(2)\n",
    "        self.fc1 = nn.Linear(8 * 8 * 8, 32)\n",
    "        self.act3 = nn.Tanh()\n",
    "        self.fc2 = nn.Linear(32, 10)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = self.pool1(self.act1(self.conv1(x)))\n",
    "        out = self.pool2(self.act2(self.conv2(out)))\n",
    "        out = out.view(-1, 8 * 8 * 8)\n",
    "        out = self.act3(self.fc1(out))\n",
    "        out = self.fc2(out)\n",
    "        return out\n",
    "\n",
    "model = Brain()\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rajouter: \n",
    "# regularization\n",
    "# dropout\n",
    "# batch normalization\n",
    "# data augmentation\n",
    "# transfer learning\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# resnet Block\n",
    "# data_augmentation\n",
    "# object_detection    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
